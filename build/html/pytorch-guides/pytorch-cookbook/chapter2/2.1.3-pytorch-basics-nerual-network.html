<!DOCTYPE html>
<html class="writer-html5" lang="zh-CN">
<head>
  <meta charset="utf-8" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0" />
  <title>3.1.14. PyTorch 基础 : 神经网络包nn和优化器optm &mdash; AI 模型国内加速  文档</title>
      <link rel="stylesheet" type="text/css" href="../../../_static/pygments.css" />
      <link rel="stylesheet" type="text/css" href="../../../_static/css/theme.css" />
      <link rel="stylesheet" type="text/css" href="../../../_static/style.css" />

  
  <!--[if lt IE 9]>
    <script src="../../../_static/js/html5shiv.min.js"></script>
  <![endif]-->
  
        <script data-url_root="../../../" id="documentation_options" src="../../../_static/documentation_options.js"></script>
        <script src="../../../_static/jquery.js"></script>
        <script src="../../../_static/underscore.js"></script>
        <script src="../../../_static/_sphinx_javascript_frameworks_compat.js"></script>
        <script src="../../../_static/doctools.js"></script>
        <script src="../../../_static/translations.js"></script>
    <script src="../../../_static/js/theme.js"></script>
    <link rel="author" title="关于这些文档" href="../../../about.html" />
    <link rel="index" title="索引" href="../../../genindex.html" />
    <link rel="search" title="搜索" href="../../../search.html" />
    <link rel="next" title="3.1.15. PyTorch 基础 :数据的加载和预处理" href="2.1.4-pytorch-basics-data-loader.html" />
    <link rel="prev" title="3.1.13. 使用PyTorch计算梯度数值" href="2.1.2-pytorch-basics-autograd.html" /> 
</head>

<body class="wy-body-for-nav"> 
  <div class="wy-grid-for-nav">
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >

          
          
          <a href="../../../index.html" class="icon icon-home">
            AI 模型国内加速
          </a>
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../../search.html" method="get">
    <input type="text" name="q" placeholder="搜索文档" aria-label="搜索文档" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>
        </div><div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="导航菜单">
              <p class="caption" role="heading"><span class="caption-text">目录:</span></p>
<ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../../../best-practice/index.html">1. 【AI网盘】人工智能资源汇总</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../stable-diffusion/index.html">2. Stable Diffusion</a></li>
<li class="toctree-l1 current"><a class="reference internal" href="../../index.html">3. Pytorch 教程</a><ul class="current">
<li class="toctree-l2 current"><a class="reference internal" href="../index.html">3.1. Pytorch 快速入门</a><ul class="current">
<li class="toctree-l3"><a class="reference internal" href="../chapter1/1.1-pytorch-introduction.html">3.1.1. 1.1 Pytorch 简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/1.2-pytorch-installation.html">3.1.2. 1.2 Pytorch环境搭建</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/1.2-pytorch-installation.html#id1">3.1.3. 1.2.1 安装Pytorch</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/1.3-deep-learning-with-pytorch-60-minute-blitz.html">3.1.4. PyTorch 深度学习:60分钟快速入门 （官方）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/1.4-pytorch-resource.html">3.1.5. 相关资源列表</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/1_tensor_tutorial.html">3.1.6. PyTorch是什么?</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/2_autograd_tutorial.html">3.1.7. Autograd: 自动求导机制</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/3_neural_networks_tutorial.html">3.1.8. Neural Networks</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/4_cifar10_tutorial.html">3.1.9. 训练一个分类器</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/5_data_parallel_tutorial.html">3.1.10. 数据并行（选读）</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter1/readme.html">3.1.11. PyTorch 中文手册第一章 ： PyTorch入门</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.1.1.pytorch-basics-tensor.html">3.1.12. PyTorch 基础 : 张量</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.1.2-pytorch-basics-autograd.html">3.1.13. 使用PyTorch计算梯度数值</a></li>
<li class="toctree-l3 current"><a class="current reference internal" href="#">3.1.14. PyTorch 基础 : 神经网络包nn和优化器optm</a><ul>
<li class="toctree-l4"><a class="reference internal" href="#id1">3.1.14.1. 定义一个网络</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id2">3.1.14.2. 损失函数</a></li>
<li class="toctree-l4"><a class="reference internal" href="#id3">3.1.14.3. 优化器</a></li>
</ul>
</li>
<li class="toctree-l3"><a class="reference internal" href="2.1.4-pytorch-basics-data-loader.html">3.1.15. PyTorch 基础 :数据的加载和预处理</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.2-deep-learning-basic-mathematics.html">3.1.16. 2.2 深度学习基础及数学原理</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.3-deep-learning-neural-network-introduction.html">3.1.17. 2.3 神经网络简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.4-cnn.html">3.1.18. 2.4 卷积神经网络简介</a></li>
<li class="toctree-l3"><a class="reference internal" href="2.5-rnn.html">3.1.19. 2.5 循环神经网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="readme.html">3.1.20. Pytorch 中文手册第二章 ： 基础</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/3.1-logistic-regression.html">3.1.21. 3.1 logistic回归实战</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/3.2-mnist.html">3.1.22. 3.2  MNIST数据集手写数字识别</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/3.3-rnn.html">3.1.23. 3.3 通过Sin预测Cos</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter3/readme.html">3.1.24. Pytorch 中文手册第三章 ： 实践</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/4.1-fine-tuning.html">3.1.25. 4.1 Fine tuning 模型微调</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/4.2.1-visdom.html">3.1.26. 4.2.1 使用Visdom在 PyTorch 中进行可视化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/4.2.2-tensorboardx.html">3.1.27. 4.2.2 使用Tensorboard在 PyTorch 中进行可视化</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/4.2.3-cnn-visualizing.html">3.1.28. 4.2.3 可视化理解卷积神经网络</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/4.2.3-cnn-visualizing.html#backpropagation">3.1.29. 基于Backpropagation的方法</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/4.3-fastai.html">3.1.30. 4.3 fastai</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/4.5-multiply-gpu-parallel-training.html">3.1.31. 4.5 多GPU并行训练</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/distributed_data_parallel.html">3.1.32. 在PyTorch中使用DistributedDataParallel进行多GPU分布式模型训练</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter4/readme.html">3.1.33. Pytorch 中文手册第四章 ： 提高</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter5/5.1-kaggle.html">3.1.34. 5.1 kaggle介绍</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter5/5.2-Structured-Data.html">3.1.35. 5.2 Pytorch处理结构化数据</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter5/5.3-Fashion-MNIST.html">3.1.36. Fashion MNIST进行分类</a></li>
<li class="toctree-l3"><a class="reference internal" href="../chapter5/readme.html">3.1.37. Pytorch 中文手册第五章 ： 应用</a></li>
</ul>
</li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="../../../style-guides/index.html">4. 编程语言 风格指南</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../coding/index.html">5. 编程语言 语法</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../cheatsheet/index.html">6. Cheat Sheet</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../../about.html">7. 关于</a></li>
</ul>

        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap"><nav class="wy-nav-top" aria-label="移动版导航菜单" >
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../../index.html">AI 模型国内加速</a>
      </nav>

      <div class="wy-nav-content">
        <div class="rst-content">
          <div role="navigation" aria-label="页面导航">
  <ul class="wy-breadcrumbs">
      <li><a href="../../../index.html" class="icon icon-home" aria-label="Home"></a></li>
          <li class="breadcrumb-item"><a href="../../index.html"><span class="section-number">3. </span>Pytorch 教程</a></li>
          <li class="breadcrumb-item"><a href="../index.html"><span class="section-number">3.1. </span>Pytorch 快速入门</a></li>
      <li class="breadcrumb-item active"><span class="section-number">3.1.14. </span>PyTorch 基础 : 神经网络包nn和优化器optm</li>
      <li class="wy-breadcrumbs-aside">
            <a href="../../../_sources/pytorch-guides/pytorch-cookbook/chapter2/2.1.3-pytorch-basics-nerual-network.md.txt" rel="nofollow"> 查看页面源码</a>
      </li>
  </ul>
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
             
  <section id="pytorch-nnoptm">
<h1><span class="section-number">3.1.14. </span>PyTorch 基础 : 神经网络包nn和优化器optm<a class="headerlink" href="#pytorch-nnoptm" title="Permalink to this heading"></a></h1>
<p>torch.nn是专门为神经网络设计的模块化接口。nn构建于 Autograd之上，可用来定义和运行神经网络。
这里我们主要介绍几个一些常用的类</p>
<p><strong>约定：torch.nn 我们为了方便使用，会为他设置别名为nn，本章除nn以外还有其他的命名约定</strong></p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="c1"># 首先要引入相关的包</span>
<span class="kn">import</span> <span class="nn">torch</span>
<span class="c1"># 引入torch.nn并指定别名</span>
<span class="kn">import</span> <span class="nn">torch.nn</span> <span class="k">as</span> <span class="nn">nn</span>
<span class="c1">#打印一下版本</span>
<span class="n">torch</span><span class="o">.</span><span class="n">__version__</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="s1">&#39;1.0.0&#39;</span>
</pre></div>
</div>
<p>除了nn别名以外，我们还引用了nn.functional，这个包中包含了神经网络中使用的一些常用函数，这些函数的特点是，不具有可学习的参数(如ReLU，pool，DropOut等)，这些函数可以放在构造函数中，也可以不放，但是这里建议不放。</p>
<p>一般情况下我们会<strong>将nn.functional 设置为大写的F</strong>，这样缩写方便调用</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.nn.functional</span> <span class="k">as</span> <span class="nn">F</span>
</pre></div>
</div>
<section id="id1">
<h2><span class="section-number">3.1.14.1. </span>定义一个网络<a class="headerlink" href="#id1" title="Permalink to this heading"></a></h2>
<p>PyTorch中已经为我们准备好了现成的网络模型，只要继承nn.Module，并实现它的forward方法，PyTorch会根据autograd，自动实现backward函数，在forward函数中可使用任何tensor支持的函数，还可以使用if、for循环、print、log等Python语法，写法和标准的Python写法一致。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">Net</span><span class="p">(</span><span class="n">nn</span><span class="o">.</span><span class="n">Module</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="c1"># nn.Module子类的函数必须在构造函数中执行父类的构造函数</span>
        <span class="nb">super</span><span class="p">(</span><span class="n">Net</span><span class="p">,</span> <span class="bp">self</span><span class="p">)</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="c1"># 卷积层 &#39;1&#39;表示输入图片为单通道， &#39;6&#39;表示输出通道数，&#39;3&#39;表示卷积核为3*3</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">3</span><span class="p">)</span> 
        <span class="c1">#线性层，输入1350个特征，输出10个特征</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span>   <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">Linear</span><span class="p">(</span><span class="mi">1350</span><span class="p">,</span> <span class="mi">10</span><span class="p">)</span>  <span class="c1">#这里的1350是如何计算的呢？这就要看后面的forward函数</span>
    <span class="c1">#正向传播 </span>
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span> 
        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># 结果：[1, 1, 32, 32]</span>
        <span class="c1"># 卷积 -&gt; 激活 -&gt; 池化 </span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">conv1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span> <span class="c1">#根据卷积的尺寸计算公式，计算结果是30，具体计算公式后面第二章第四节 卷积神经网络 有详细介绍。</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># 结果：[1, 6, 30, 30]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">max_pool2d</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="p">(</span><span class="mi">2</span><span class="p">,</span> <span class="mi">2</span><span class="p">))</span> <span class="c1">#我们使用池化层，计算结果是15</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">F</span><span class="o">.</span><span class="n">relu</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>
        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># 结果：[1, 6, 15, 15]</span>
        <span class="c1"># reshape，‘-1’表示自适应</span>
        <span class="c1">#这里做的就是压扁的操作 就是把后面的[1, 6, 15, 15]压扁，变为 [1, 1350]</span>
        <span class="n">x</span> <span class="o">=</span> <span class="n">x</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">()[</span><span class="mi">0</span><span class="p">],</span> <span class="o">-</span><span class="mi">1</span><span class="p">)</span> 
        <span class="nb">print</span><span class="p">(</span><span class="n">x</span><span class="o">.</span><span class="n">size</span><span class="p">())</span> <span class="c1"># 这里就是fc1层的的输入1350 </span>
        <span class="n">x</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">fc1</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>        
        <span class="k">return</span> <span class="n">x</span>

<span class="n">net</span> <span class="o">=</span> <span class="n">Net</span><span class="p">()</span>
<span class="nb">print</span><span class="p">(</span><span class="n">net</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Net</span><span class="p">(</span>
  <span class="p">(</span><span class="n">conv1</span><span class="p">):</span> <span class="n">Conv2d</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="n">kernel_size</span><span class="o">=</span><span class="p">(</span><span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">),</span> <span class="n">stride</span><span class="o">=</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">))</span>
  <span class="p">(</span><span class="n">fc1</span><span class="p">):</span> <span class="n">Linear</span><span class="p">(</span><span class="n">in_features</span><span class="o">=</span><span class="mi">1350</span><span class="p">,</span> <span class="n">out_features</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">bias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="p">)</span>
</pre></div>
</div>
<p>网络的可学习参数通过net.parameters()返回</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">parameters</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">parameters</span><span class="p">)</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[[[</span> <span class="mf">0.2745</span><span class="p">,</span>  <span class="mf">0.2594</span><span class="p">,</span>  <span class="mf">0.0171</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0429</span><span class="p">,</span>  <span class="mf">0.3013</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0208</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.1459</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3223</span><span class="p">,</span>  <span class="mf">0.1797</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">0.1847</span><span class="p">,</span>  <span class="mf">0.0227</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1919</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">0.0210</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1336</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2176</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">0.2164</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1244</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2428</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">0.1042</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0055</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2171</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.3306</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2808</span><span class="p">,</span>  <span class="mf">0.2058</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.2492</span><span class="p">,</span>  <span class="mf">0.2971</span><span class="p">,</span>  <span class="mf">0.2277</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">0.2134</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0644</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3044</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0040</span><span class="p">,</span>  <span class="mf">0.0828</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2093</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0204</span><span class="p">,</span>  <span class="mf">0.1065</span><span class="p">,</span>  <span class="mf">0.1168</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">0.1651</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2244</span><span class="p">,</span>  <span class="mf">0.3072</span><span class="p">],</span>
          <span class="p">[</span><span class="o">-</span><span class="mf">0.2301</span><span class="p">,</span>  <span class="mf">0.2443</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.2340</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0685</span><span class="p">,</span>  <span class="mf">0.1026</span><span class="p">,</span>  <span class="mf">0.1754</span><span class="p">]]],</span>


        <span class="p">[[[</span> <span class="mf">0.1691</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0790</span><span class="p">,</span>  <span class="mf">0.2617</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.1956</span><span class="p">,</span>  <span class="mf">0.1477</span><span class="p">,</span>  <span class="mf">0.0877</span><span class="p">],</span>
          <span class="p">[</span> <span class="mf">0.0538</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3091</span><span class="p">,</span>  <span class="mf">0.2030</span><span class="p">]]]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.2355</span><span class="p">,</span>  <span class="mf">0.2949</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.1283</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0848</span><span class="p">,</span>  <span class="mf">0.2027</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.3331</span><span class="p">],</span>
       <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([[</span> <span class="mf">2.0555e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.1445e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.7981e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3864e-02</span><span class="p">,</span>
          <span class="mf">8.5149e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">6.2071e-04</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.1755e-02</span><span class="p">,</span>  <span class="mf">1.0010e-02</span><span class="p">,</span>  <span class="mf">2.1978e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">1.8433e-02</span><span class="p">,</span>
          <span class="mf">7.1362e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.0951e-03</span><span class="p">],</span>
        <span class="p">[</span> <span class="mf">1.6187e-02</span><span class="p">,</span>  <span class="mf">2.1623e-02</span><span class="p">,</span>  <span class="mf">1.1840e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">5.7059e-03</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">2.7165e-02</span><span class="p">,</span>  <span class="mf">1.3463e-03</span><span class="p">],</span>
        <span class="o">...</span><span class="p">,</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">3.2552e-03</span><span class="p">,</span>  <span class="mf">1.7277e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.4907e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">7.4232e-03</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">2.7188e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">4.6431e-03</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">1.9786e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">3.7382e-03</span><span class="p">,</span>  <span class="mf">1.2259e-02</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span>  <span class="mf">3.2471e-03</span><span class="p">,</span>
         <span class="o">-</span><span class="mf">1.2375e-02</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.6372e-02</span><span class="p">],</span>
        <span class="p">[</span><span class="o">-</span><span class="mf">8.2350e-03</span><span class="p">,</span>  <span class="mf">4.1301e-03</span><span class="p">,</span> <span class="o">-</span><span class="mf">1.9192e-03</span><span class="p">,</span>  <span class="o">...</span><span class="p">,</span> <span class="o">-</span><span class="mf">2.3119e-05</span><span class="p">,</span>
          <span class="mf">2.0167e-03</span><span class="p">,</span>  <span class="mf">1.9528e-02</span><span class="p">]],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">Parameter</span> <span class="n">containing</span><span class="p">:</span>
<span class="n">tensor</span><span class="p">([</span> <span class="mf">0.0162</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0146</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0218</span><span class="p">,</span>  <span class="mf">0.0212</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0119</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0142</span><span class="p">,</span> <span class="o">-</span><span class="mf">0.0079</span><span class="p">,</span>  <span class="mf">0.0171</span><span class="p">,</span>
         <span class="mf">0.0205</span><span class="p">,</span>  <span class="mf">0.0164</span><span class="p">],</span> <span class="n">requires_grad</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
</pre></div>
</div>
<p>net.named_parameters可同时返回可学习的参数及名称。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="k">for</span> <span class="n">name</span><span class="p">,</span><span class="n">parameters</span> <span class="ow">in</span> <span class="n">net</span><span class="o">.</span><span class="n">named_parameters</span><span class="p">():</span>
    <span class="nb">print</span><span class="p">(</span><span class="n">name</span><span class="p">,</span><span class="s1">&#39;:&#39;</span><span class="p">,</span><span class="n">parameters</span><span class="o">.</span><span class="n">size</span><span class="p">())</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">conv1</span><span class="o">.</span><span class="n">weight</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">3</span><span class="p">,</span> <span class="mi">3</span><span class="p">])</span>
<span class="n">conv1</span><span class="o">.</span><span class="n">bias</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">6</span><span class="p">])</span>
<span class="n">fc1</span><span class="o">.</span><span class="n">weight</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">,</span> <span class="mi">1350</span><span class="p">])</span>
<span class="n">fc1</span><span class="o">.</span><span class="n">bias</span> <span class="p">:</span> <span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<p>forward函数的输入和输出都是Tensor</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">randn</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">)</span> <span class="c1"># 这里的对应前面fforward的输入是32</span>
<span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span>
<span class="n">out</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1350</span><span class="p">])</span>





<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">10</span><span class="p">])</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="nb">input</span><span class="o">.</span><span class="n">size</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
</pre></div>
</div>
<p>在反向传播前，先要将所有参数的梯度清零</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">net</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
<span class="n">out</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">torch</span><span class="o">.</span><span class="n">ones</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">))</span> <span class="c1"># 反向传播的实现是PyTorch自动实现的，我们只要调用这个函数即可</span>
</pre></div>
</div>
<p><strong>注意</strong>:torch.nn只支持mini-batches，不支持一次只输入一个样本，即一次必须是一个batch。</p>
<p>也就是说，就算我们输入一个样本，也会对样本进行分批，所以，所有的输入都会增加一个维度，我们对比下刚才的input，nn中定义为3维，但是我们人工创建时多增加了一个维度，变为了4维，最前面的1即为batch-size</p>
</section>
<section id="id2">
<h2><span class="section-number">3.1.14.2. </span>损失函数<a class="headerlink" href="#id2" title="Permalink to this heading"></a></h2>
<p>在nn中PyTorch还预制了常用的损失函数，下面我们用MSELoss用来计算均方误差</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">y</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">view</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span><span class="mi">10</span><span class="p">)</span><span class="o">.</span><span class="n">float</span><span class="p">()</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">#loss是个scalar，我们可以直接用item获取到他的python类型的数值</span>
<span class="nb">print</span><span class="p">(</span><span class="n">loss</span><span class="o">.</span><span class="n">item</span><span class="p">())</span> 
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="mf">28.92203712463379</span>
</pre></div>
</div>
</section>
<section id="id3">
<h2><span class="section-number">3.1.14.3. </span>优化器<a class="headerlink" href="#id3" title="Permalink to this heading"></a></h2>
<p>在反向传播计算完所有参数的梯度后，还需要使用优化方法来更新网络的权重和参数，例如随机梯度下降法(SGD)的更新策略如下：</p>
<p>weight = weight - learning_rate * gradient</p>
<p>在torch.optim中实现大多数的优化方法，例如RMSProp、Adam、SGD等，下面我们使用SGD做个简单的样例</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch.optim</span>
</pre></div>
</div>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span><span class="n">out</span> <span class="o">=</span> <span class="n">net</span><span class="p">(</span><span class="nb">input</span><span class="p">)</span> <span class="c1"># 这里调用的时候会打印出我们在forword函数中打印的x的大小</span>
<span class="n">criterion</span> <span class="o">=</span> <span class="n">nn</span><span class="o">.</span><span class="n">MSELoss</span><span class="p">()</span>
<span class="n">loss</span> <span class="o">=</span> <span class="n">criterion</span><span class="p">(</span><span class="n">out</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="c1">#新建一个优化器，SGD只需要要调整的参数和学习率</span>
<span class="n">optimizer</span> <span class="o">=</span> <span class="n">torch</span><span class="o">.</span><span class="n">optim</span><span class="o">.</span><span class="n">SGD</span><span class="p">(</span><span class="n">net</span><span class="o">.</span><span class="n">parameters</span><span class="p">(),</span> <span class="n">lr</span> <span class="o">=</span> <span class="mf">0.01</span><span class="p">)</span>
<span class="c1"># 先梯度清零(与net.zero_grad()效果一样)</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">zero_grad</span><span class="p">()</span> 
<span class="n">loss</span><span class="o">.</span><span class="n">backward</span><span class="p">()</span>

<span class="c1">#更新参数</span>
<span class="n">optimizer</span><span class="o">.</span><span class="n">step</span><span class="p">()</span>
</pre></div>
</div>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span><span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span> <span class="mi">32</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">30</span><span class="p">,</span> <span class="mi">30</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">6</span><span class="p">,</span> <span class="mi">15</span><span class="p">,</span> <span class="mi">15</span><span class="p">])</span>
<span class="n">torch</span><span class="o">.</span><span class="n">Size</span><span class="p">([</span><span class="mi">1</span><span class="p">,</span> <span class="mi">1350</span><span class="p">])</span>
</pre></div>
</div>
<p>这样，神经网络的数据的一个完整的传播就已经通过PyTorch实现了，下面一章将介绍PyTorch提供的数据加载和处理工具，使用这些工具可以方便的处理所需要的数据。</p>
<p>看完这节，大家可能对神经网络模型里面的一些参数的计算方式还有疑惑，这部分会在第二章 第四节 卷积神经网络有详细介绍，并且在第三章 第二节 MNIST数据集手写数字识别的实践代码中有详细的注释说明。</p>
<div class="highlight-python notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</section>
</section>


           </div>
          </div>
          <footer><div class="rst-footer-buttons" role="navigation" aria-label="页脚">
        <a href="2.1.2-pytorch-basics-autograd.html" class="btn btn-neutral float-left" title="3.1.13. 使用PyTorch计算梯度数值" accesskey="p" rel="prev"><span class="fa fa-arrow-circle-left" aria-hidden="true"></span> 上一页</a>
        <a href="2.1.4-pytorch-basics-data-loader.html" class="btn btn-neutral float-right" title="3.1.15. PyTorch 基础 :数据的加载和预处理" accesskey="n" rel="next">下一页 <span class="fa fa-arrow-circle-right" aria-hidden="true"></span></a>
    </div>

  <hr/>

  <div role="contentinfo">
    <p>&#169; 版权所有 2024.</p>
  </div>

  利用 <a href="https://www.sphinx-doc.org/">Sphinx</a> 构建，使用的 
    <a href="https://github.com/readthedocs/sphinx_rtd_theme">主题</a>
    由 <a href="https://readthedocs.org">Read the Docs</a> 开发.
  
<div class="view_counter">
      <img class="img_view_counter" src="https://s01.flagcounter.com/count2/m02K/bg_FFFFFF/txt_F77B1B/border_CCCCCC/columns_3/maxflags_6/viewers_3/labels_1/pageviews_0/flags_0/percent_0/" alt="View Counter" border="0" />
</div>


</footer>
        </div>
      </div>
    </section>
  </div>
  <script>
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>
<div class="rst-versions" data-toggle="rst-versions" role="note" aria-label="版本">
    <span class="rst-current-version" data-toggle="rst-current-version">
        <span class="fa fa-book"> Read the Docs</span>
        v: latest
        <span class="fa fa-caret-down"></span>
    </span>
    <div class="rst-other-versions">
        <dl>
            <dt>版本</dt>
            <dd><a href="#">latest</a></dd>
        </dl>
    </div>
</div>

</body>
</html>